# Key implementations in official code

## From the paper [PGGAN]

### Progressive training
* All existing layers in both networks remain trainable throughout the training process.
* ![][PGGAN-fig01]
* When new layers are added to the networks, we fade them in smoothly.
* ![][PGGAN-fig02]
* alpha is flipped in official code
* `implementation_alpha = (1.0 - paper_alpha)`


### Minibatch standard deviations
* Adding a minibatch layer towards the end of the discriminator.
```python
def minibatch_stddev_layer(x, group_size=4, num_new_features=1):
    with tf.variable_scope('MinibatchStddev'):
        group_size = tf.minimum(group_size, tf.shape(x)[0])
        s = x.shape
        y = tf.reshape(x, [group_size, -1, num_new_features, s[1] // num_new_features, s[2], s[3]])
        y = tf.cast(y, tf.float32)
        y -= tf.reduce_mean(y, axis=0, keepdims=True)
        y = tf.reduce_mean(tf.square(y), axis=0)
        y = tf.sqrt(y + 1e-8)
        y = tf.reduce_mean(y, axis=[2, 3, 4], keepdims=True)
        y = tf.reduce_mean(y, axis=[2])
        y = tf.cast(y, x.dtype)
        y = tf.tile(y, [group_size, 1, s[2], s[3]])
        return tf.concat([x, y], axis=1)
```

### Equalized learning rate
* initialize weights via `N(0, 1)` but scales with per-layer normalization constant from Heâ€™s initializer.
* `w^_i = w_i / c`
* `c = gain * sqrt(1.0/number_of_inputs)` 

```python
def get_weight(weight_shape, gain, lrmul):
    fan_in = np.prod(weight_shape[:-1])  # [kernel, kernel, fmaps_in, fmaps_out] or [in, out]
    he_std = gain / np.sqrt(fan_in)  # He init

    # equalized learning rate
    init_std = 1.0 / lrmul
    runtime_coef = he_std * lrmul

    # create variable.
    weight = tf.get_variable('weight', shape=weight_shape, dtype=tf.float32,
                             initializer=tf.initializers.random_normal(0, init_std)) * runtime_coef
    return weight
```
    

## From the paper [StyleGAN]

### Generator architecture
* Style-based Generator
* ![][StyleGAN-fig01]

### AdaIN
* Learned affine transformations then specialize w to styles `y = (y_s, y_b)` that control adaptive instance normalization (AdaIN) operations after each convolution layer of the synthesis network g.
* ![][StyleGAN-eq01]
```python
def instance_norm(x, epsilon=1e-8):
    # x: [?, 512, h, w]
    assert len(x.shape) == 4  # NCHW
    with tf.variable_scope('InstanceNorm'):
        epsilon = tf.constant(epsilon, dtype=tf.float32, name='epsilon')

        # tf.reduce_mean(x, axis=[2, 3], keepdims=True): [?, 512, 1, 1]
        # x: [?, 512, h, w]
        x = x - tf.reduce_mean(x, axis=[2, 3], keepdims=True)

        # tf.reduce_mean(tf.square(x), axis=[2, 3], keepdims=True): [?, 512, 1, 1]
        # x: [?, 512, h, w]
        x = x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=[2, 3], keepdims=True) + epsilon)
    return x


def style_mod(x, w):
    # x: [?, 512, h, w]
    # w: [?, 512]
    with tf.variable_scope('StyleMod'):
        # units: 1024
        units = x.shape[1] * 2

        # style: [?, 1024]
        style = equalized_dense(w, units, gain=1.0, lrmul=1.0)
        style = apply_bias(style, lrmul=1.0)

        # style: [?, 2, 512, 1, 1]
        style = tf.reshape(style, [-1, 2, x.shape[1]] + [1] * (len(x.shape) - 2))

        # x * (style[:, 0] + 1): [?, 512, h, w]
        # x: [?, 512, h, w]
        x = x * (style[:, 0] + 1) + style[:, 1]
    return x


def adaptive_instance_norm(x, w):
    x = instance_norm(x)
    x = style_mod(x, w)
    return x
```

### Style mixing regularizations
* To further encourage the styles to localize, we employ mixing regularization, where a given percentage of images are generated using two random latent codes instead of one during training.
```python
def style_mixing_regularization(z, w_dim, w_broadcasted, n_mapping, n_broadcast,
                                train_res_block_idx, style_mixing_prob):
    with tf.name_scope('StyleMix'):
        z2 = tf.random_normal(tf.shape(z), dtype=tf.float32)
        w_broadcasted2 = g_mapping(z2, w_dim, n_mapping, n_broadcast)
        layer_indices = np.arange(n_broadcast)[np.newaxis, :, np.newaxis]
        last_layer_index = (train_res_block_idx + 1) * 2
        mixing_cutoff = tf.cond(
            tf.random_uniform([], 0.0, 1.0) < style_mixing_prob,
            lambda: tf.random_uniform([], 1, last_layer_index, dtype=tf.int32),
            lambda: tf.constant(last_layer_index, dtype=tf.int32))
        w_broadcasted = tf.where(tf.broadcast_to(layer_indices < mixing_cutoff, tf.shape(w_broadcasted)),
                                 w_broadcasted,
                                 w_broadcasted2)
    return w_broadcasted
```

### Truncation trick in w
* drawing latent vectors from a truncated or otherwise shrunk sampling
space tends to improve average image quality, although some amount of variation is lost.

```python
def truncation_trick(n_broadcast, w_broadcasted, w_avg, truncation_psi, truncation_cutoff):
    with tf.variable_scope('Truncation'):
        layer_indices = np.arange(n_broadcast)[np.newaxis, :, np.newaxis]
        ones = np.ones(layer_indices.shape, dtype=np.float32)
        coefs = tf.where(layer_indices < truncation_cutoff, truncation_psi * ones, ones)
        w_broadcasted = lerp(w_avg, w_broadcasted, coefs)
    return w_broadcasted
```


[PGGAN]: https://arxiv.org/abs/1710.10196
[StyleGAN]: https://arxiv.org/abs/1812.04948
[PGGAN-fig01]: ./assets/PGGAN-fig01.png
[PGGAN-fig02]: ./assets/PGGAN-fig02.png
[StyleGAN-fig01]: ./assets/StyleGAN-fig01.png
[StyleGAN-eq01]: ./assets/StyleGAN-eq01.png